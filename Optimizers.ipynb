{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# <div style=\"text-align: center; background-color: #3498db; padding: 10px; border-radius: 10px;\">Optimizers</div>\n"
      ],
      "metadata": {
        "id": "8dBZhzPEmWW1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Objective:** Assess understanding of optimization algorithms in artificial neural networks. Evaluate the\n",
        "application and comparison of different optimizers. Enhance knowledge of optimizers' impact on model\n",
        "convergence and performance."
      ],
      "metadata": {
        "id": "OLSMMVwNmfOL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part 1: Understanding of Optimizers`\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "MBHmQtVmmfQX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Q1. What is the role of optimization algorithms in artificial neural networks Why are they necessary ?"
      ],
      "metadata": {
        "id": "hEoXClyOmfTO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In artificial neural networks optimizers play a very important role as they are used to minimize the loss and maximize the accuracy of the model. Optimizers basically adjusting the weights and biases of the neural networks during the training process. There are sevral type of the optimizers available for us;\n",
        "\n",
        "1. Gradient Descent\n",
        "2. Stochastic Gradient Descent\n",
        "3. Mini Batch Gradient Descent\n",
        "4. Momentum Gradeint Descent\n",
        "5. Adagrad\n",
        "6. RMS Prop\n",
        "7. Adam Optimizer"
      ],
      "metadata": {
        "id": "XdpfbzbRmfVC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Q2. Explain the concept of gradient descent and its variants. Discuss their differences and tradeoffs in terms of convergence speed and memory requirements ?"
      ],
      "metadata": {
        "id": "j2r3AJuWmfZU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Gradient descent is an optimizer which is used to minimizing the error.It minimizes the function till the range of lowest error.this function is the loss function that measures the difference between the predicted output and the actual target values.The basic idea is to calculate the gradient (partial derivative) of the loss function with respect to each model parameter and update the parameters in the opposite direction of the gradient. This process is repeated until the algorithm converges to a minimum of the loss function.\n",
        "\n",
        "**Variants and Tradeoffs:**\n",
        "\n",
        "- **Batch Gradient Descent:** Slow for large datasets, high memory requirements.\n",
        "- **Stochastic Gradient Descent (SGD):** Fast but noisy updates, low memory requirements.\n",
        "- **Mini-Batch Gradient Descent:** Balanced approach.\n",
        "- **Momentum:** Reduces oscillations, accelerates convergence.\n",
        "- **Adagrad:** Memory-intensive, sensitive to initial learning rates.\n",
        "- **RMSprop:** Addresses Adagrad's issues, more stable.\n",
        "- **Adam:** Adaptive learning rates, less sensitive to hyperparameters, often preferred for various tasks."
      ],
      "metadata": {
        "id": "WLURHhSZmfdy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Q3. Describe the challenges associated with traditional gradient descent optimization methods (e.g., slow convergence, local minima). How do modern optimizers address these challenges ?\n"
      ],
      "metadata": {
        "id": "SHBESx3jmffd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- **Slow Convergence:** Batch Gradient Descent is slow for large datasets.\n",
        "- **Local Minima:** Prone to getting stuck in local minima, leading to suboptimal solutions.\n",
        "- **Sensitivity to Learning Rates:** Choosing the right learning rate is challenging.\n",
        "- **Saddle Points:** Can slow down or halt convergence.\n",
        "\n",
        "**Modern Optimizers and Solutions:**\n",
        "\n",
        "**Stochastic Gradient Descent (SGD):** Speeds up convergence by updating parameters based on individual examples.\n",
        "\n",
        "**Mini-Batch Gradient Descent:** Balances speed and efficiency with batch updates.\n",
        "\n",
        "**Momentum:** Accelerates convergence by accumulating past gradients.\n",
        "\n",
        "Adaptive Learning Rate Methods (RMSprop, Adam): Dynamically adjust learning rates, improving stability.\n",
        "\n",
        "**Batch Normalization:** Addresses vanishing/exploding gradients, contributes to stability.\n",
        "\n",
        "**Learning Rate Schedules:** Gradually decrease learning rates to fine-tune training.\n",
        "\n",
        "**Skip Connections and Residual Networks:** Mitigate vanishing gradient problem, aiding convergence.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "K0tPMKjvmfi_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Q4. Discuss the concepts of momentum and learning rate in the context of optimization algorithms. How do they impact convergence and model performance ?"
      ],
      "metadata": {
        "id": "dZlnvhkTmfkr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "Momentum is an optimization technique that accelerates convergence by adding a fraction of the previous update to the current update, helping to maintain consistent movement towards the minimum. It reduces oscillations and speeds up convergence.\n",
        "\n",
        "\n",
        "The learning rate is a hyperparameter that determines the step size during optimization. A higher learning rate can speed up convergence, but if too high, it may cause divergence or overshooting. Balancing the learning rate is crucial for achieving optimal convergence and model performance.\n",
        "\n"
      ],
      "metadata": {
        "id": "9s9VgaXFmfpL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part 2: Optimizer Techniques\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "6mvm2tXamfq6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Q5. Explain the concept of Stochastic radient Descent (SGD) and its advantages compared to traditional gradient descent. Discuss its limitations and scenarios where it is most suitable ?"
      ],
      "metadata": {
        "id": "n8l8hB7-mfug"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "SGD updates model parameters based on the gradient of the loss for a single randomly chosen training example.\n",
        "\n",
        "**Advantages:**\n",
        "\n",
        "- Faster convergence, especially for large datasets.\n",
        "- Reduced memory requirements.\n",
        "- Escapes local minima, aiding non-convex optimization.\n",
        "- Acts as a form of regularization.\n",
        "\n",
        "**Limitations:**\n",
        "\n",
        "- Noisy updates may cause fluctuations.\n",
        "- Variability in convergence path.\n",
        "- Requires careful learning rate tuning.\n",
        "- Potential for overshooting due to stochastic updates.\n",
        "\n",
        "**Suitable Scenarios:**\n",
        "\n",
        "- Large datasets.\n",
        "- Online learning.\n",
        "- Non-convex optimization.\n",
        "- Memory-constrained environments.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "5AXXx48Zo49C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Q6. Describe the concept of Adam optimizer and how it combines momentum and adaptive learning rates. Discuss its benefits and potential drawbacks ?\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "gJf77MCgpS9p"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Key Features:**\n",
        "\n",
        "**Momentum Component:** Incorporates a momentum term similar to the one in SGD, allowing the optimizer to accumulate past gradients.\n",
        "\n",
        "**Adaptive Learning Rates:** Scales the learning rates for each parameter individually based on the historical information of squared gradients, providing adaptive updates.\n",
        "\n",
        "**Benefits:**\n",
        "\n",
        "**Efficient Convergence:** Adam often converges faster than traditional optimization methods due to the combination of momentum and adaptive learning rates.\n",
        "\n",
        "**Robust to Hyperparameters:** Adam is less sensitive to the choice of hyperparameters compared to some other optimizers, making it suitable for various tasks.\n",
        "\n",
        "**Sparse Gradients Handling:** Effectively handles sparse gradients, making it suitable for tasks with sparse data.\n",
        "Potential Drawbacks:\n",
        "\n",
        "**Memory Usage:** Adam maintains moving averages for each parameter, potentially leading to higher memory requirements compared to simpler optimizers.\n",
        "\n",
        "**Not Always the Best:** While Adam is versatile, it might not always outperform other optimizers, and its performance can depend on the specific characteristics of the optimization problem."
      ],
      "metadata": {
        "id": "WvchrU6VpS6O"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Q7. Explain the concept of RMSprop optimizer and how it addresses the challenges of adaptive learning rates. ompare it with Adam and discuss their relative strengths and weaknesses."
      ],
      "metadata": {
        "id": "hMy63myRpS4f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "RMSprop and Adam are optimization algorithms that both use adaptive learning rates to efficiently update model parameters during training. RMSprop scales learning rates individually for each parameter based on the root mean square of past squared gradients, while Adam combines this adaptive learning rate strategy with a momentum term. RMSprop is more memory-efficient but may suffer from slow convergence in certain scenarios. Adam is versatile and often performs well across various optimization problems, but it comes with higher memory usage. The choice between RMSprop and Adam depends on factors such as memory constraints and the characteristics of the optimization problem.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "bcvaqtrDqe9t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part 3: Applying Optimizer\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "goCDOAf1qesk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Q8 Implement SGD, Adam, and RMSprop optimizers in a deep learning model using a framework of your choice. Train the model on a suitable dataset and compare their impact on model convergence and performance ?"
      ],
      "metadata": {
        "id": "lW1rpCERq4R8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras"
      ],
      "metadata": {
        "id": "rBe8pngjooSO"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.datasets import mnist"
      ],
      "metadata": {
        "id": "5ivYDTcirMjg"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# spliting the data into train and test data\n",
        "(x_train_full ,y_train_full),(x_test ,y_test) = mnist.load_data()"
      ],
      "metadata": {
        "id": "oR0NZ8eDrkXW"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_train_full.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uow2YXGFs5Tl",
        "outputId": "f0d78b13-b391-455d-904e-f58feb054afe"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(60000, 28, 28)"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# defining the validation data\n",
        "x_valid = x_tarin_full[:5000]\n",
        "y_valid= y_train_full[:5000]"
      ],
      "metadata": {
        "id": "QnXgXqLvsKTH"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(x_valid.shape)\n",
        "print(y_valid.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "63_dB-1Wtiea",
        "outputId": "9ff7cf69-1e0c-4af0-adcf-3ee932f5ec9d"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(5000, 28, 28)\n",
            "(5000,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# resassigning the train data\n",
        "x_train = x_train_full[5000:]\n",
        "y_train = y_train_full[5000:]"
      ],
      "metadata": {
        "id": "fj4eYs_GtkNa"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(x_train.shape)\n",
        "print(y_train.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jitDmpNtuD7Z",
        "outputId": "101e8ed4-0a66-4dd0-944b-9fa4a6fba666"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(55000, 28, 28)\n",
            "(55000,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x_train.max()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G5leaF1Fu5kL",
        "outputId": "29c79f91-6d33-4fe9-cbe7-fdc82a85cf5f"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "255"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# normalizing the x data\n",
        "x_train,x_valid, x_test = x_train / 255,x_valid/255 ,x_test / 255"
      ],
      "metadata": {
        "id": "Yzg1Cx9quKGY"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "layers= [\n",
        "        tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
        "    tf.keras.layers.Dense(128, activation='relu'),\n",
        "    tf.keras.layers.Dense(10, activation='softmax')\n",
        "]\n"
      ],
      "metadata": {
        "id": "PRuHFjv7ukl4"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_classifier= tf.keras.models.Sequential(layers)"
      ],
      "metadata": {
        "id": "1kYe2i25vRgt"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Using SGD Optimizer"
      ],
      "metadata": {
        "id": "Sw_43ByejKRB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_classifier.compile(optimizer='SGD', loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "YNf3ZvKqhsjY"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_classifier.fit(x_train,y_train , epochs=20 , validation_data= (x_valid,y_valid))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ScrV3cI2iSjW",
        "outputId": "40e64bad-dc80-491b-920f-837d4ad66688"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "1719/1719 [==============================] - 7s 3ms/step - loss: 0.6681 - accuracy: 0.8361 - val_loss: 0.3659 - val_accuracy: 0.8996\n",
            "Epoch 2/20\n",
            "1719/1719 [==============================] - 5s 3ms/step - loss: 0.3474 - accuracy: 0.9026 - val_loss: 0.2984 - val_accuracy: 0.9188\n",
            "Epoch 3/20\n",
            "1719/1719 [==============================] - 5s 3ms/step - loss: 0.2988 - accuracy: 0.9159 - val_loss: 0.2653 - val_accuracy: 0.9272\n",
            "Epoch 4/20\n",
            "1719/1719 [==============================] - 5s 3ms/step - loss: 0.2694 - accuracy: 0.9243 - val_loss: 0.2422 - val_accuracy: 0.9344\n",
            "Epoch 5/20\n",
            "1719/1719 [==============================] - 5s 3ms/step - loss: 0.2471 - accuracy: 0.9299 - val_loss: 0.2236 - val_accuracy: 0.9378\n",
            "Epoch 6/20\n",
            "1719/1719 [==============================] - 5s 3ms/step - loss: 0.2283 - accuracy: 0.9359 - val_loss: 0.2094 - val_accuracy: 0.9428\n",
            "Epoch 7/20\n",
            "1719/1719 [==============================] - 5s 3ms/step - loss: 0.2126 - accuracy: 0.9405 - val_loss: 0.1979 - val_accuracy: 0.9450\n",
            "Epoch 8/20\n",
            "1719/1719 [==============================] - 5s 3ms/step - loss: 0.1987 - accuracy: 0.9440 - val_loss: 0.1854 - val_accuracy: 0.9478\n",
            "Epoch 9/20\n",
            "1719/1719 [==============================] - 6s 3ms/step - loss: 0.1865 - accuracy: 0.9475 - val_loss: 0.1764 - val_accuracy: 0.9522\n",
            "Epoch 10/20\n",
            "1719/1719 [==============================] - 5s 3ms/step - loss: 0.1760 - accuracy: 0.9502 - val_loss: 0.1670 - val_accuracy: 0.9550\n",
            "Epoch 11/20\n",
            "1719/1719 [==============================] - 5s 3ms/step - loss: 0.1663 - accuracy: 0.9533 - val_loss: 0.1616 - val_accuracy: 0.9568\n",
            "Epoch 12/20\n",
            "1719/1719 [==============================] - 5s 3ms/step - loss: 0.1577 - accuracy: 0.9557 - val_loss: 0.1537 - val_accuracy: 0.9598\n",
            "Epoch 13/20\n",
            "1719/1719 [==============================] - 5s 3ms/step - loss: 0.1500 - accuracy: 0.9575 - val_loss: 0.1474 - val_accuracy: 0.9596\n",
            "Epoch 14/20\n",
            "1719/1719 [==============================] - 5s 3ms/step - loss: 0.1431 - accuracy: 0.9600 - val_loss: 0.1408 - val_accuracy: 0.9618\n",
            "Epoch 15/20\n",
            "1719/1719 [==============================] - 6s 4ms/step - loss: 0.1365 - accuracy: 0.9616 - val_loss: 0.1396 - val_accuracy: 0.9608\n",
            "Epoch 16/20\n",
            "1719/1719 [==============================] - 5s 3ms/step - loss: 0.1308 - accuracy: 0.9633 - val_loss: 0.1323 - val_accuracy: 0.9634\n",
            "Epoch 17/20\n",
            "1719/1719 [==============================] - 5s 3ms/step - loss: 0.1255 - accuracy: 0.9649 - val_loss: 0.1277 - val_accuracy: 0.9650\n",
            "Epoch 18/20\n",
            "1719/1719 [==============================] - 5s 3ms/step - loss: 0.1207 - accuracy: 0.9665 - val_loss: 0.1250 - val_accuracy: 0.9650\n",
            "Epoch 19/20\n",
            "1719/1719 [==============================] - 5s 3ms/step - loss: 0.1160 - accuracy: 0.9672 - val_loss: 0.1216 - val_accuracy: 0.9676\n",
            "Epoch 20/20\n",
            "1719/1719 [==============================] - 5s 3ms/step - loss: 0.1118 - accuracy: 0.9688 - val_loss: 0.1197 - val_accuracy: 0.9678\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7d6c708c31c0>"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Using ADAM optimizer"
      ],
      "metadata": {
        "id": "ZyKStLFXi-l4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_classifier.compile(optimizer='Adam', loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "Lw3M2dakixjW"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_classifier.fit(x_train,y_train , epochs=20 , validation_data= (x_valid,y_valid))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ptNI4sLdjbRc",
        "outputId": "af126f15-6dbb-4e1f-bbb6-5891b74e9f42"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "1719/1719 [==============================] - 7s 3ms/step - loss: 0.1221 - accuracy: 0.9633 - val_loss: 0.1043 - val_accuracy: 0.9690\n",
            "Epoch 2/20\n",
            "1719/1719 [==============================] - 5s 3ms/step - loss: 0.0823 - accuracy: 0.9749 - val_loss: 0.0852 - val_accuracy: 0.9752\n",
            "Epoch 3/20\n",
            "1719/1719 [==============================] - 5s 3ms/step - loss: 0.0590 - accuracy: 0.9818 - val_loss: 0.0890 - val_accuracy: 0.9722\n",
            "Epoch 4/20\n",
            "1719/1719 [==============================] - 5s 3ms/step - loss: 0.0470 - accuracy: 0.9850 - val_loss: 0.0848 - val_accuracy: 0.9752\n",
            "Epoch 5/20\n",
            "1719/1719 [==============================] - 5s 3ms/step - loss: 0.0362 - accuracy: 0.9884 - val_loss: 0.0862 - val_accuracy: 0.9758\n",
            "Epoch 6/20\n",
            "1719/1719 [==============================] - 6s 4ms/step - loss: 0.0286 - accuracy: 0.9912 - val_loss: 0.0795 - val_accuracy: 0.9776\n",
            "Epoch 7/20\n",
            "1719/1719 [==============================] - 5s 3ms/step - loss: 0.0238 - accuracy: 0.9922 - val_loss: 0.0810 - val_accuracy: 0.9776\n",
            "Epoch 8/20\n",
            "1719/1719 [==============================] - 5s 3ms/step - loss: 0.0199 - accuracy: 0.9940 - val_loss: 0.0895 - val_accuracy: 0.9772\n",
            "Epoch 9/20\n",
            "1719/1719 [==============================] - 5s 3ms/step - loss: 0.0161 - accuracy: 0.9952 - val_loss: 0.0909 - val_accuracy: 0.9780\n",
            "Epoch 10/20\n",
            "1719/1719 [==============================] - 5s 3ms/step - loss: 0.0133 - accuracy: 0.9959 - val_loss: 0.0884 - val_accuracy: 0.9784\n",
            "Epoch 11/20\n",
            "1719/1719 [==============================] - 5s 3ms/step - loss: 0.0124 - accuracy: 0.9959 - val_loss: 0.0982 - val_accuracy: 0.9770\n",
            "Epoch 12/20\n",
            "1719/1719 [==============================] - 6s 4ms/step - loss: 0.0102 - accuracy: 0.9967 - val_loss: 0.0922 - val_accuracy: 0.9764\n",
            "Epoch 13/20\n",
            "1719/1719 [==============================] - 5s 3ms/step - loss: 0.0096 - accuracy: 0.9967 - val_loss: 0.0957 - val_accuracy: 0.9810\n",
            "Epoch 14/20\n",
            "1719/1719 [==============================] - 5s 3ms/step - loss: 0.0083 - accuracy: 0.9974 - val_loss: 0.0871 - val_accuracy: 0.9816\n",
            "Epoch 15/20\n",
            "1719/1719 [==============================] - 5s 3ms/step - loss: 0.0071 - accuracy: 0.9978 - val_loss: 0.1131 - val_accuracy: 0.9774\n",
            "Epoch 16/20\n",
            "1719/1719 [==============================] - 5s 3ms/step - loss: 0.0075 - accuracy: 0.9974 - val_loss: 0.1267 - val_accuracy: 0.9742\n",
            "Epoch 17/20\n",
            "1719/1719 [==============================] - 5s 3ms/step - loss: 0.0078 - accuracy: 0.9973 - val_loss: 0.1103 - val_accuracy: 0.9774\n",
            "Epoch 18/20\n",
            "1719/1719 [==============================] - 5s 3ms/step - loss: 0.0078 - accuracy: 0.9974 - val_loss: 0.1065 - val_accuracy: 0.9768\n",
            "Epoch 19/20\n",
            "1719/1719 [==============================] - 5s 3ms/step - loss: 0.0053 - accuracy: 0.9984 - val_loss: 0.1078 - val_accuracy: 0.9794\n",
            "Epoch 20/20\n",
            "1719/1719 [==============================] - 5s 3ms/step - loss: 0.0046 - accuracy: 0.9986 - val_loss: 0.1118 - val_accuracy: 0.9786\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7d6c6efbe4d0>"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Using RMSprop Optimizer"
      ],
      "metadata": {
        "id": "TN8qs6Y9ji86"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_classifier.compile(optimizer='rmsprop', loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "hTNWg7qZjhH-"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_classifier.fit(x_train,y_train , epochs=20 , validation_data= (x_valid,y_valid))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H-Z2Sk6YjvTs",
        "outputId": "d882560a-c958-495b-e24a-c7bb5fa767c1"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "1719/1719 [==============================] - 6s 3ms/step - loss: 0.0011 - accuracy: 0.9998 - val_loss: 0.1019 - val_accuracy: 0.9820\n",
            "Epoch 2/20\n",
            "1719/1719 [==============================] - 5s 3ms/step - loss: 2.8053e-04 - accuracy: 1.0000 - val_loss: 0.1032 - val_accuracy: 0.9840\n",
            "Epoch 3/20\n",
            "1719/1719 [==============================] - 5s 3ms/step - loss: 1.7288e-04 - accuracy: 1.0000 - val_loss: 0.1035 - val_accuracy: 0.9842\n",
            "Epoch 4/20\n",
            "1719/1719 [==============================] - 5s 3ms/step - loss: 1.0810e-04 - accuracy: 1.0000 - val_loss: 0.1023 - val_accuracy: 0.9822\n",
            "Epoch 5/20\n",
            "1719/1719 [==============================] - 5s 3ms/step - loss: 8.1617e-05 - accuracy: 1.0000 - val_loss: 0.1046 - val_accuracy: 0.9834\n",
            "Epoch 6/20\n",
            "1719/1719 [==============================] - 5s 3ms/step - loss: 6.5874e-05 - accuracy: 1.0000 - val_loss: 0.1056 - val_accuracy: 0.9832\n",
            "Epoch 7/20\n",
            "1719/1719 [==============================] - 5s 3ms/step - loss: 5.8959e-05 - accuracy: 1.0000 - val_loss: 0.1058 - val_accuracy: 0.9826\n",
            "Epoch 8/20\n",
            "1719/1719 [==============================] - 5s 3ms/step - loss: 5.0858e-05 - accuracy: 1.0000 - val_loss: 0.1067 - val_accuracy: 0.9832\n",
            "Epoch 9/20\n",
            "1719/1719 [==============================] - 5s 3ms/step - loss: 4.3877e-05 - accuracy: 1.0000 - val_loss: 0.1068 - val_accuracy: 0.9836\n",
            "Epoch 10/20\n",
            "1719/1719 [==============================] - 6s 4ms/step - loss: 3.9783e-05 - accuracy: 1.0000 - val_loss: 0.1086 - val_accuracy: 0.9828\n",
            "Epoch 11/20\n",
            "1719/1719 [==============================] - 5s 3ms/step - loss: 3.6299e-05 - accuracy: 1.0000 - val_loss: 0.1072 - val_accuracy: 0.9836\n",
            "Epoch 12/20\n",
            "1719/1719 [==============================] - 5s 3ms/step - loss: 3.3286e-05 - accuracy: 1.0000 - val_loss: 0.1076 - val_accuracy: 0.9838\n",
            "Epoch 13/20\n",
            "1719/1719 [==============================] - 5s 3ms/step - loss: 3.1152e-05 - accuracy: 1.0000 - val_loss: 0.1089 - val_accuracy: 0.9834\n",
            "Epoch 14/20\n",
            "1719/1719 [==============================] - 5s 3ms/step - loss: 2.9396e-05 - accuracy: 1.0000 - val_loss: 0.1102 - val_accuracy: 0.9824\n",
            "Epoch 15/20\n",
            "1719/1719 [==============================] - 5s 3ms/step - loss: 2.7565e-05 - accuracy: 1.0000 - val_loss: 0.1095 - val_accuracy: 0.9830\n",
            "Epoch 16/20\n",
            "1719/1719 [==============================] - 5s 3ms/step - loss: 2.6192e-05 - accuracy: 1.0000 - val_loss: 0.1105 - val_accuracy: 0.9834\n",
            "Epoch 17/20\n",
            "1719/1719 [==============================] - 5s 3ms/step - loss: 2.4453e-05 - accuracy: 1.0000 - val_loss: 0.1113 - val_accuracy: 0.9828\n",
            "Epoch 18/20\n",
            "1719/1719 [==============================] - 5s 3ms/step - loss: 2.3363e-05 - accuracy: 1.0000 - val_loss: 0.1108 - val_accuracy: 0.9832\n",
            "Epoch 19/20\n",
            "1719/1719 [==============================] - 5s 3ms/step - loss: 2.2072e-05 - accuracy: 1.0000 - val_loss: 0.1124 - val_accuracy: 0.9824\n",
            "Epoch 20/20\n",
            "1719/1719 [==============================] - 6s 3ms/step - loss: 2.1299e-05 - accuracy: 1.0000 - val_loss: 0.1112 - val_accuracy: 0.9830\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7d6c6eecd810>"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Q9. Discuss the considerations and tradeoffs when choosing the appropriate optimizer for a given neural network architecture and task. onsider factors such as convergence speed, stability, and generalization performance."
      ],
      "metadata": {
        "id": "vH8mPtoOkxGL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- **Convergence Speed:** Adaptive learning rate optimizers (e.g., Adam) can speed up convergence.\n",
        "- **Stability:** Momentum-based optimizers (e.g., Adam) enhance stability.\n",
        "- **Generalization:** Regularization techniques and adaptive learning rate optimizers contribute to better generalization.\n",
        "- **Memory Requirements:** Consider memory-efficient optimizers for large models (e.g., RMSprop).\n",
        "- **Hyperparameter Sensitivity:** Some optimizers (e.g., Adam) are less sensitive to hyperparameter choices.\n"
      ],
      "metadata": {
        "id": "2OrEBywAkfaf"
      }
    }
  ]
}